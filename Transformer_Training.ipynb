{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.embedding = nn.Embedding(num_embeddings=ntoken, embedding_dim=d_model)\n",
        "        self.d_model = d_model\n",
        "        self.linear = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            src: Tensor, shape ``[batch_size, seq_len]``\n",
        "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
        "        \"\"\"\n",
        "        src = src.to(torch.long)\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        if src_mask is None:\n",
        "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "            \"\"\"\n",
        "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src[0])).to(device)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "iiieKbBriwF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWH-Rcdu65-J",
        "outputId": "4c0c4f00-2615-4a88-8df7-c2dc4012b7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "417\n",
            "8838561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/972 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/100. loss:1.7151; val loss: 1.6134; acc: 0.5247; val acc: 0.5496\n",
            "\n",
            "Epoch 2/100. loss:1.7142; val loss: 1.6145; acc: 0.5248; val acc: 0.5491\n",
            "\n",
            "Epoch 3/100. loss:1.7140; val loss: 1.6132; acc: 0.5249; val acc: 0.5498\n",
            "\n",
            "Epoch 4/100. loss:1.7136; val loss: 1.6131; acc: 0.5251; val acc: 0.5493\n",
            "\n",
            "Epoch 5/100. loss:1.7128; val loss: 1.6114; acc: 0.5252; val acc: 0.5504\n",
            "\n",
            "Epoch 6/100. loss:1.7124; val loss: 1.6119; acc: 0.5253; val acc: 0.5503\n",
            "\n",
            "Epoch 7/100. loss:1.7117; val loss: 1.6113; acc: 0.5253; val acc: 0.5501\n",
            "\n",
            "Epoch 8/100. loss:1.7114; val loss: 1.6101; acc: 0.5255; val acc: 0.5507\n",
            "\n",
            "Epoch 9/100. loss:1.7107; val loss: 1.6104; acc: 0.5257; val acc: 0.5499\n",
            "\n",
            "Epoch 10/100. loss:1.7102; val loss: 1.6101; acc: 0.5258; val acc: 0.5501\n",
            "\n",
            "Epoch 11/100. loss:1.7094; val loss: 1.6094; acc: 0.5259; val acc: 0.5508\n",
            "\n",
            "Epoch 12/100. loss:1.7090; val loss: 1.6091; acc: 0.5260; val acc: 0.5503\n",
            "\n",
            "Epoch 13/100. loss:1.7085; val loss: 1.6094; acc: 0.5261; val acc: 0.5502\n",
            "\n",
            "Epoch 14/100. loss:1.7079; val loss: 1.6078; acc: 0.5263; val acc: 0.5512\n",
            "\n",
            "Epoch 15/100. loss:1.7074; val loss: 1.6064; acc: 0.5264; val acc: 0.5515\n",
            "\n",
            "Epoch 16/100. loss:1.7067; val loss: 1.6055; acc: 0.5265; val acc: 0.5518\n",
            "\n",
            "Epoch 17/100. loss:1.7062; val loss: 1.6052; acc: 0.5266; val acc: 0.5514\n",
            "\n",
            "Epoch 18/100. loss:1.7060; val loss: 1.6045; acc: 0.5267; val acc: 0.5520\n",
            "\n",
            "Epoch 19/100. loss:1.7051; val loss: 1.6062; acc: 0.5269; val acc: 0.5506\n",
            "\n",
            "Epoch 20/100. loss:1.7046; val loss: 1.6055; acc: 0.5269; val acc: 0.5508\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "root = 'drive/MyDrive/Colab_Notebooks/Torch/'\n",
        "root = ''\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.long\n",
        "filepath = root + 'numerized_dataset512.json'\n",
        "# hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 1e-5\n",
        "LOSS_EVERY = 100\n",
        "# get vocab size\n",
        "\n",
        "with open(root + 'token_map.json', 'r') as f:\n",
        "    token_map = json.load(f)\n",
        "    VOCAB_SIZE = len(token_map) + 2 # +1 for padding token, +1 for start token\n",
        "print(VOCAB_SIZE)\n",
        "# SEQ_LENGTH: to change seq_length, run preprocess_dataset with desired SEQ_LENGTH\n",
        "\n",
        "# load dataset\n",
        "def load_dataset(filepath):\n",
        "    '''\n",
        "    :return: each train/val/test data is an array of sequences [[...], [...] ...]\n",
        "    '''\n",
        "    with open(filepath, 'r') as f:\n",
        "        dataset = json.load(f)\n",
        "        train_data = []\n",
        "        val_data = []\n",
        "        test_data = []\n",
        "        for sequence_pair in dataset['train']:\n",
        "            train_data.append(sequence_pair[0])\n",
        "        for sequence_pair in dataset['val']:\n",
        "            val_data.append(sequence_pair[0])\n",
        "        for sequence_pair in dataset['test']:\n",
        "            test_data.append(sequence_pair[0])\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def train_batch(batch, targets):\n",
        "    '''\n",
        "    :param batch: sequence shifted right, shape (batch_size, seq_length)\n",
        "    :param targets: unshifted sequence, shape (batch_size, seq_length)\n",
        "    :return:\n",
        "    '''\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss() # shapes: input (N, num_classes, d1); target (N, d1); returns (N, d1)\n",
        "                                    # so we have to permute logit dimensions\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    logits = model(batch) # logits have shape (batch_size, seq_length, vocab_size)\n",
        "    loss = loss_fn(logits.permute(0, 2, 1), targets).sum()\n",
        "\n",
        "    accuracy = torch.eq(logits.argmax(dim=2, keepdim=False), targets).float().sum()/\\\n",
        "                       (targets.shape[0]*targets.shape[1])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss, accuracy\n",
        "\n",
        "def get_input_sequences(batch):\n",
        "    '''\n",
        "    :param batch: shape (batch_size, seq_length)\n",
        "    remove last token in each sequence, prepend start token to each sequence\n",
        "    '''\n",
        "    batch = np.array(batch)\n",
        "    batch -= 1\n",
        "    batch_size, seq_length = batch.shape\n",
        "    # Remove the last token from each sequence\n",
        "    batch_trimmed = batch[:, :-1]\n",
        "    # Prepend start token (int = vocab_size) to each sequence\n",
        "    start_token = np.ones((batch_size, 1))*(VOCAB_SIZE-1)  # Replace this with the actual start token\n",
        "    input_data = np.concatenate([start_token, batch_trimmed], axis=1)\n",
        "    #print(input_data[0].tolist())\n",
        "    return input_data\n",
        "\n",
        "#==============================================================================================================\n",
        "\n",
        "train_data, val_data, test_data = load_dataset(filepath)\n",
        "num_iterations = len(train_data)//(BATCH_SIZE)\n",
        "EPOCH = 15\n",
        "model = TransformerModel(ntoken=VOCAB_SIZE, d_model=512, nhead=4, d_hid=1024, nlayers=4, dropout=0.1)\n",
        "model.to(device)\n",
        "state_dict = torch.load(f'test8_epoch{EPOCH}_sd.pth', map_location='cuda')\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(num_params)\n",
        "\n",
        "t = tqdm(range(num_iterations))\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    for i in range(num_iterations):\n",
        "        batch = train_data[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
        "        batch_loss, batch_accuracy = train_batch(torch.tensor(get_input_sequences(batch), dtype=dtype, device=device),\n",
        "                                                 torch.tensor((np.array(batch)-1), dtype=dtype, device=device))\n",
        "\n",
        "        epoch_loss += batch_loss\n",
        "        epoch_accuracy += batch_accuracy\n",
        "\n",
        "        if i % LOSS_EVERY == 0 and i > 0:\n",
        "            train_loss = epoch_loss/(i+1)\n",
        "            train_accuracy = epoch_accuracy/(i+1)\n",
        "            #t.set_description(f\"Epoch: {epoch}: Iteration: {i} Loss: {train_loss:.4f} Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    # get validation stats on epoch end\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_iterations = len(val_data)//BATCH_SIZE\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        for j in range(val_iterations):\n",
        "            batch = val_data[j * BATCH_SIZE:(j + 1) * BATCH_SIZE]\n",
        "            targets = torch.tensor((np.array(batch)-1), dtype=dtype, device=device)\n",
        "            #batch.to(device)\n",
        "            #targets.to(device)\n",
        "\n",
        "            logits = model(torch.tensor(get_input_sequences(batch), dtype=dtype, device=device))  # logits have shape (batch_size, seq_length, vocab_size)\n",
        "            loss = loss_fn(logits.permute(0, 2, 1), targets).sum()\n",
        "\n",
        "            accuracy = torch.eq(logits.argmax(dim=2, keepdim=False), targets).float().sum()/\\\n",
        "                       (targets.shape[0]*targets.shape[1])\n",
        "\n",
        "            #print(logits.argmax(dim=2))\n",
        "\n",
        "            val_loss += loss\n",
        "            val_accuracy += accuracy\n",
        "\n",
        "        val_loss /= (j+1)\n",
        "        val_accuracy /= (j+1)\n",
        "\n",
        "        print(f'\\nEpoch {epoch+1}/{EPOCHS}. loss:{train_loss:.4f}; val loss: {val_loss:.4f}; acc: {train_accuracy:.4f}; val acc: {val_accuracy:.4f}')\n",
        "        # save model\n",
        "        model_path = f'epoch{epoch+1+EPOCH}_sd.pth'\n",
        "        torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = TransformerModel(ntoken=VOCAB_SIZE, d_model=512, nhead=4, d_hid=1024, nlayers=4, dropout=0.1)\n",
        "model.to(device)\n",
        "state_dict = torch.load('test4_epoch21_sd.pth', map_location='cuda')\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "inputs = [[416]]\n",
        "outputs = []\n",
        "softmax = nn.softmax(dim=2)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for i in range(50):\n",
        "    pred_logits = model(torch.tensor(inputs, dtype=torch.long, device=device))\n",
        "    pred_probs = softmax(pred_logits)\n",
        "    pred_index = torch.multinomial(pred_probs[0, i], num_samples=1, replacement=True)\n",
        "    inputs[0].append(pred_index)\n",
        "    outputs.append(pred_index)\n",
        "  print(outputs)\n"
      ],
      "metadata": {
        "id": "J5ZH4dqeM7zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "def create_mask(type, d):\n",
        "  '''\n",
        "  :param type {1, 2, 3}\n",
        "  type 1: for input layer\n",
        "  type 2: for hidden layers\n",
        "  type 3: for output layer\n",
        "  type 4: matrix for converting x -> y\n",
        "  :return:\n",
        "  '''\n",
        "  mask = []\n",
        "\n",
        "  if type == 1:\n",
        "    for i in range(2 * d):\n",
        "      new_row = []\n",
        "      for j in range(d):\n",
        "        if j * 2 + 1 < i:\n",
        "          new_row.append(1)\n",
        "        else:\n",
        "          new_row.append(0)\n",
        "      mask.append(new_row)\n",
        "    mask = torch.tensor(mask, requires_grad=False).type(torch.float32).to(device)\n",
        "\n",
        "  elif type == 2:\n",
        "    for i in range(d):\n",
        "      new_row = []\n",
        "      for j in range(d):\n",
        "        if j <= i:\n",
        "          new_row.append(1)\n",
        "        else:\n",
        "          new_row.append(0)\n",
        "      mask.append(new_row)\n",
        "    print(np.array(mask))\n",
        "    mask = torch.tensor(mask, requires_grad=False).type(torch.float32).to(device)\n",
        "\n",
        "  elif type == 3:\n",
        "    for i in range(d):\n",
        "      new_row = []\n",
        "      for j in range(2 * d):\n",
        "        if j <= i * 2 + 1:\n",
        "          new_row.append(1)\n",
        "        else:\n",
        "          new_row.append(0)\n",
        "      mask.append(new_row)\n",
        "    mask = torch.tensor(mask, requires_grad=False).type(torch.float32).to(device)\n",
        "\n",
        "  elif type == 4:\n",
        "    for i in range(2 * d):\n",
        "      new_row = []\n",
        "      for j in range(d):\n",
        "        if j * 2 <= i and i % 2 != 0 and j * 2 + 1 == i:\n",
        "          new_row.append(1)\n",
        "        else:\n",
        "          new_row.append(0)\n",
        "      mask.append(new_row)\n",
        "    mask = np.array(mask)\n",
        "  return mask\n",
        "\n",
        "create_mask(2, 6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "9RVOXF2LY7H9",
        "outputId": "2319a1ac-93ea-49b3-e2d8-9da320af9fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 0 0 0]\n",
            " [1 1 0 0 0 0]\n",
            " [1 1 1 0 0 0]\n",
            " [1 1 1 1 0 0]\n",
            " [1 1 1 1 1 0]\n",
            " [1 1 1 1 1 1]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'device' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-36bf7d940e1b>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-36bf7d940e1b>\u001b[0m in \u001b[0;36mcreate_mask\u001b[0;34m(type, d)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.arange(5)\n",
        "print(x)\n",
        "print(x[2:4])\n",
        "print(x.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdxcG7CsaaJD",
        "outputId": "2fbc6c8f-cf0d-4960-f432-b4872cf764cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4])\n",
            "tensor([2, 3])\n",
            "torch.int64\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}